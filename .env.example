# Git Terminal Assistant - LLM Configuration
# Copy this file to .env and configure your preferred LLM provider

# LLM Provider Selection
# Options: ollama, openai, anthropic, google, cohere, azure_openai
LLM_PROVIDER=ollama

# Default model for the selected provider
# Examples:
# - Ollama: llama3.1:8b, mistral, gemma:2b
# - OpenAI: gpt-4, gpt-4-turbo-preview, gpt-3.5-turbo
# - Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229
# - Google: gemini-pro, gemini-1.5-pro
# - Cohere: command, command-light
LLM_MODEL=llama3.1:8b

# Provider-specific settings

# OpenAI
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_API_BASE=https://api.openai.com/v1  # Optional, for custom endpoints
OPENAI_ORGANIZATION=  # Optional

# Anthropic
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Google (Vertex AI or Generative AI)
GOOGLE_API_KEY=your-google-api-key-here
# Or for Vertex AI:
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
# GOOGLE_PROJECT_ID=your-project-id

# Cohere
COHERE_API_KEY=your-cohere-api-key-here

# Azure OpenAI
AZURE_OPENAI_API_KEY=your-azure-openai-key-here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Ollama (for local models)
OLLAMA_HOST=http://localhost:11434  # Default Ollama server

# Advanced Settings
LLM_TEMPERATURE=0.3  # Model temperature (0.0-1.0)
LLM_MAX_TOKENS=2000  # Maximum tokens for response
LLM_TIMEOUT=60  # Request timeout in seconds

# Agent Routing Strategy
# Options: llm, heuristic
# - llm: Use LLM for intelligent intent classification (recommended)
# - heuristic: Use manual keyword detection (faster, less accurate)
GTA_ROUTER=llm
GTA_ROUTER_THRESHOLD=0.7  # Confidence threshold for LLM routing (0.0-1.0)

# Per-agent model overrides (optional)
# Leave empty to use the default model for all agents
GIT_AGENT_MODEL=
CODE_AGENT_MODEL=
CHAT_AGENT_MODEL=

# Fallback settings
# If primary provider fails, try these in order
FALLBACK_PROVIDERS=ollama  # Comma-separated list
FALLBACK_MODEL=llama3.1:8b  # Model to use for fallback

# Testing and Coverage
# Control how pipelines and test runs handle coverage.
# Artifacts will be written under .gta/coverage/ (coverage.xml and summary.json).
# Requires pytest and pytest-cov installed in your project environment.

# Enable coverage during multi-agent pipelines (tests + commit/message flows)
# 1/true/on to enable, 0/false/off to disable
GTA_PIPELINES_RUN_COVERAGE=0

# Percentage threshold (0-100). Used when gating commits on coverage.
GTA_COVERAGE_THRESHOLD=80

# Require tests to pass before committing in pipelines (default: enabled)
GTA_COMMIT_REQUIRE_TESTS_PASS=1

# Require coverage to meet threshold before committing in pipelines (default: disabled)
GTA_COMMIT_REQUIRE_COVERAGE_PASS=0

# Enable coverage when CodeAgent runs tests outside pipelines
# If enabled, pytest will be invoked with coverage and artifacts saved to .gta/coverage/
GTA_TESTS_ENABLE_COVERAGE=0

## Reasoning and Perception System Configuration

# Chain-of-Thought (CoT) reasoning mode for planning
# none = no structured reasoning, brief = 3-7 step plan, structured = full JSON trace
GTA_REASONING_MODE=structured

# Enable/disable the reasoning system
GTA_REASONING_ENABLED=1

# Save reasoning traces to .orchestra/runs/
GTA_SAVE_TRACES=1

# Enable proactive perception system
GTA_PERCEPTION_ENABLED=1
GTA_PERCEPTION_COOLDOWN=300
GTA_PERCEPTION_SILENT=0
GTA_PERCEPTION_AUTO_DISMISS=30
GTA_PERCEPTION_MAX_CONCURRENT=3
GTA_PERCEPTION_ANALYSIS_INTERVAL=30
GTA_FS_DEBOUNCE_INTERVAL=0.5
GTA_FS_BUFFER_FLUSH_INTERVAL=2.0
GTA_GIT_CHECK_INTERVAL=15
