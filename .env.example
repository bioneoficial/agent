# Git Terminal Assistant - LLM Configuration
# Copy this file to .env and configure your preferred LLM provider

# LLM Provider Selection
# Options: ollama, openai, anthropic, google, cohere, azure_openai
LLM_PROVIDER=ollama

# Default model for the selected provider
# Examples:
# - Ollama: llama3.1:8b, mistral, gemma:2b
# - OpenAI: gpt-4, gpt-4-turbo-preview, gpt-3.5-turbo
# - Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229
# - Google: gemini-pro, gemini-1.5-pro
# - Cohere: command, command-light
LLM_MODEL=llama3.1:8b

# Provider-specific settings

# OpenAI
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_API_BASE=https://api.openai.com/v1  # Optional, for custom endpoints
OPENAI_ORGANIZATION=  # Optional

# Anthropic
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Google (Vertex AI or Generative AI)
GOOGLE_API_KEY=your-google-api-key-here
# Or for Vertex AI:
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
# GOOGLE_PROJECT_ID=your-project-id

# Cohere
COHERE_API_KEY=your-cohere-api-key-here

# Azure OpenAI
AZURE_OPENAI_API_KEY=your-azure-openai-key-here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Ollama (for local models)
OLLAMA_HOST=http://localhost:11434  # Default Ollama server

# Advanced Settings
LLM_TEMPERATURE=0.3  # Model temperature (0.0-1.0)
LLM_MAX_TOKENS=2000  # Maximum tokens for response
LLM_TIMEOUT=60  # Request timeout in seconds

# Per-agent model overrides (optional)
# Leave empty to use the default model for all agents
GIT_AGENT_MODEL=
CODE_AGENT_MODEL=
CHAT_AGENT_MODEL=

# Fallback settings
# If primary provider fails, try these in order
FALLBACK_PROVIDERS=ollama  # Comma-separated list
FALLBACK_MODEL=llama3.1:8b  # Model to use for fallback
